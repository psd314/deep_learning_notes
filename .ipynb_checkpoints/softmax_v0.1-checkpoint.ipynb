{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "# import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, preprocessing\n",
    "# from gc_utils import dictionary_to_vector, vector_to_dictionary, gradients_to_vector\n",
    "\n",
    "# from deep_nn_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: gradient_check_n\n",
    "# will need dimensions fixed\n",
    "def dictionary_to_vector(parameters):\n",
    "    \"\"\"\n",
    "    Roll all our parameters dictionary into a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    keys = []\n",
    "    count = 0\n",
    "    for key in [\"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\", \"W4\", \"b4\"]:\n",
    "        \n",
    "        # flatten parameter\n",
    "        new_vector = np.reshape(parameters[key], (-1,1))\n",
    "        keys = keys + [key]*new_vector.shape[0]\n",
    "        \n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "\n",
    "    return theta, keys\n",
    "\n",
    "def vector_to_dictionary(theta):\n",
    "    \"\"\"\n",
    "    Unroll all our parameters dictionary from a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    parameters[\"W1\"] = theta[:12].reshape((3,4))      # 4, 3, 8, 5, 3\n",
    "    parameters[\"b1\"] = theta[12:15].reshape((3,1))\n",
    "    parameters[\"W2\"] = theta[15:39].reshape((8,3))\n",
    "    parameters[\"b2\"] = theta[39:47].reshape((8,1))\n",
    "    parameters[\"W3\"] = theta[47:87].reshape((5,8))\n",
    "    parameters[\"b3\"] = theta[87:92].reshape((5,1))\n",
    "    parameters[\"W4\"] = theta[92:107].reshape((3,5))\n",
    "    parameters[\"b4\"] = theta[107:110].reshape((3,1))\n",
    "    return parameters\n",
    "\n",
    "def gradients_to_vector(gradients):\n",
    "    \"\"\"\n",
    "    Roll all our gradients dictionary into a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    \n",
    "    count = 0\n",
    "    for key in [\"dW1\", \"db1\", \"dW2\", \"db2\", \"dW3\", \"db3\", \"dW4\", \"db4\"]:\n",
    "        # flatten parameter\n",
    "        new_vector = np.reshape(gradients[key], (-1,1))\n",
    "        \n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "\n",
    "    return theta\n",
    "\n",
    "def gradient_check_n(parameters, gradients, X, Y, epsilon = 1e-7):\n",
    "    \"\"\"\n",
    "    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters. \n",
    "    x -- input datapoint, of shape (input size, 1)\n",
    "    y -- true \"label\"\n",
    "    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n",
    "    \n",
    "    Returns:\n",
    "    difference -- difference (2) between the approximated gradient and the backward propagation gradient\n",
    "    \"\"\"\n",
    "\n",
    "    # Set-up variables\n",
    "    parameters_values, _ = dictionary_to_vector(parameters)\n",
    "    grad = gradients_to_vector(gradients)\n",
    "    num_parameters = parameters_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "    \n",
    "    # Compute gradapprox\n",
    "    for i in range(num_parameters):\n",
    "        \n",
    "        # Compute J_plus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_plus[i]\".\n",
    "        # \"_\" is used because the function you have to outputs two parameters but we only care about the first one\n",
    "        ### START CODE HERE ### (approx. 3 lines)\n",
    "        thetaplus = np.copy(parameters_values)                                      # Step 1\n",
    "        thetaplus[i][0] = thetaplus[i][0] + epsilon                                # Step 2\n",
    "        AL_plus, _ = L_model_forward(X, vector_to_dictionary(thetaplus))\n",
    "        J_plus[i] = compute_cost(AL_plus, Y)\n",
    "#         J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))                                   # Step 3\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute J_minus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_minus[i]\".\n",
    "        ### START CODE HERE ### (approx. 3 lines)\n",
    "        thetaminus = np.copy(parameters_values)                                     # Step 1\n",
    "        thetaminus[i][0] = thetaminus[i][0] - epsilon                               # Step 2   \n",
    "        AL_minus, _ = L_model_forward(X, vector_to_dictionary(thetaminus))\n",
    "        J_minus[i] = compute_cost(AL_minus, Y)\n",
    "#         J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus))                                   # Step 3\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute gradapprox[i]\n",
    "        ### START CODE HERE ### (approx. 1 line)\n",
    "        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Compare gradapprox to backward propagation gradients by computing difference.\n",
    "    ### START CODE HERE ### (approx. 1 line)\n",
    "    numerator = np.linalg.norm(grad - gradapprox)                                            # Step 1'\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)                                         # Step 2'\n",
    "    difference = numerator / denominator                                           # Step 3'\n",
    "    ### END CODE HERE ###\n",
    "#     print('grad')\n",
    "#     print(grad)\n",
    "#     print('grad approx')\n",
    "#     print(gradapprox)\n",
    "#     print('numerator', numerator)\n",
    "#     print('denominator', denominator)\n",
    "#     print(grad-gradapprox)\n",
    "    if difference > 1.2e-7:\n",
    "        print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    else:\n",
    "        print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "# layers_dims: array containing dimensions of each layer ex initialize_parameters([5, 4, 3])\n",
    "\n",
    "    np.random.seed(12)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in nn\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "#     print([(k, parameters[k].shape) for k, v in parameters.items()])\n",
    "#     for k, v in parameters.items():\n",
    "#         print(k, v.shape)\n",
    "    return parameters\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "        \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "\n",
    "    assert(A.shape == Z.shape)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def softmax(Z):\n",
    "    t = np.exp(Z)\n",
    "    A = t / np.sum(t, axis=0)\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "    \n",
    "\n",
    "def neuron_activation(A_prev, W, b, activation):\n",
    "    if activation == 'sigmoid':\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        # linear cache: A, W, b\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        # activation cache: Z\n",
    "        \n",
    "    elif activation == 'relu':\n",
    "#         print('relu')\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    elif activation == 'softmax':\n",
    "#         print('softmax')\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "        \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2 # // is floor division\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        \n",
    "        A_prev = A\n",
    "        \n",
    "        A, cache = neuron_activation(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], 'relu')\n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache = neuron_activation(A, parameters['W' + str(L)], parameters['b' + str(L)], 'softmax')\n",
    "    caches.append(cache)\n",
    "#     print('\\n', AL)\n",
    "#     assert(AL.shape == (1, X.shape[1]))\n",
    "    assert(AL.shape == (3, X.shape[1])) # should be  (classes x m)\n",
    "\n",
    "    return AL, caches\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "#     cost = (-1/m) * np.sum(Y*np.log(AL) + (1-Y)*np.log(1-AL))\n",
    "    \n",
    "#     cost = np.squeeze(cost)\n",
    "#     assert(cost.shape == ())\n",
    "#     cost = (-1/m) * np.sum(Y * np.log(AL))\n",
    "    cost = (-1/m) * np.sum(Y * np.log(AL))\n",
    "    \n",
    "    return cost\n",
    "    \n",
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "#     print('dZ.shape', dZ.shape, 'A_prev.shape', A_prev.shape, 'W.shape', W.shape)\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "#     dW = (1/m) * np.dot(A_prev, dZ.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "#     dA_prev = np.dot(W, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1-s) # dLoss/dA * a(1-a) -- same as dLoss/dA * g_prime(Z)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def softmax_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = dA\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    # check here for reverse grad shape bug\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "       \n",
    "    elif activation == 'softmax':\n",
    "        dZ = softmax_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "#     Y = Y.reshape(AL.shape)\n",
    "    \n",
    "#     dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) #derivative of cost function\n",
    "    dAL = AL - Y # this is dZ for softmax, not dA * g_prime(z)\n",
    "    current_cache = caches[L-1]\n",
    "\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"softmax\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+2)], current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l+1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l+1)] = dW_temp\n",
    "        grads[\"db\" + str(l+1)] = db_temp\n",
    "            \n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2      \n",
    "    for l in range(1, L+1):\n",
    "        parameters['W'+str(l)] = parameters['W'+str(l)] - learning_rate * grads['dW'+str(l)]\n",
    "        parameters['b'+str(l)] = parameters['b'+str(l)] - learning_rate * grads['db'+str(l)]\n",
    "#     print('update parameters')\n",
    "#     for k, v in parameters.items():\n",
    "#         print(k, v)\n",
    "    return parameters\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    print('X', X.shape, 'Y.shape', y.shape)\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2\n",
    "    p = np.zeros((y.shape[0], m))\n",
    "    acc = []\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "    print('probas\\n', probas.shape)\n",
    "    # max for each example\n",
    "    # search for index == max\n",
    "    # assign 1 to that class and 0 to the others\n",
    "    for i in range(0, probas.shape[1]):\n",
    "#         print(probas[:, i])\n",
    "        index = np.where(probas[:, i] == np.max(probas[:, i]))\n",
    "        probas[index, i] = 1\n",
    "        zeros = np.where(probas[:, i] != 1)\n",
    "        probas[zeros, i] = 0\n",
    "        if np.where(probas[:,i] == 1) == np.where(y[:,i] ==1):\n",
    "            acc.append(True)\n",
    "            \n",
    "    print(type(probas))\n",
    "    print(\"Accuracy: \" + str(sum(acc)/m))\n",
    "#     print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layer_dims, learning_rate = .0075, num_iterations = 10, print_cost = False):\n",
    "    np.random.seed(3)\n",
    "    costs = []\n",
    "    \n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        cost = compute_cost(AL, Y)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 10000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "#             difference = gradient_check_n(parameters, grads, X, Y)\n",
    "#             print(difference)\n",
    "        if print_cost and i % 10000 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = datasets.load_wine(True)\n",
    "wine[0].shape[0]\n",
    "X = wine[0].T\n",
    "X = preprocessing.scale(X)\n",
    "\n",
    "Y_cat = wine[1]\n",
    "Y = []\n",
    "for i in Y_cat:\n",
    "    if i==0:\n",
    "        Y.append([1,0,0])\n",
    "    elif i == 1:\n",
    "        Y.append([0,1,0])\n",
    "    elif i == 2:\n",
    "        Y.append([0,0,1])\n",
    "Y = np.array(Y).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris = datasets.load_iris(True)\n",
    "# iris[0].shape[0]\n",
    "# X = iris[0].T\n",
    "# X = preprocessing.scale(X)\n",
    "\n",
    "# Y_cat = iris[1]\n",
    "# Y = []\n",
    "# for i in Y_cat:\n",
    "#     if i==0:\n",
    "#         Y.append([1,0,0])\n",
    "#     elif i == 1:\n",
    "#         Y.append([0,1,0])\n",
    "#     elif i == 2:\n",
    "#         Y.append([0,0,1])\n",
    "# Y = np.array(Y).T\n",
    "# split into training/test\n",
    "# Y = Y[:, :5]\n",
    "# print('X.shape', X.shape)\n",
    "# print('Y.shape', Y.shape)\n",
    "# iris\n",
    "#normalize data\n",
    "# l2 regularization\n",
    "# gradient checking\n",
    "# X.mean(axis=0)\n",
    "# X.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 3000 # number of points per class\n",
    "# D = 2 # dimensionality\n",
    "# K = 3 # number of classes\n",
    "# X = np.zeros((N*K,D)) # data matrix (each row = single example)\n",
    "# y = np.zeros(N*K, dtype='uint8') # class labels\n",
    "# for j in range(K):\n",
    "#   ix = range(N*j,N*(j+1))\n",
    "#   r = np.linspace(0.0,1,N) # radius\n",
    "#   t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
    "#   X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "#   y[ix] = j\n",
    "# # lets visualize the data:\n",
    "# plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "# plt.show()\n",
    "# X = X.T\n",
    "# Y = []\n",
    "# for i in y:\n",
    "#     if i==0:\n",
    "#         Y.append([1,0,0])\n",
    "#     elif i == 1:\n",
    "#         Y.append([0,1,0])\n",
    "#     elif i == 2:\n",
    "#         Y.append([0,0,1])\n",
    "# Y = np.array(Y).T\n",
    "# print(Y.shape, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(101)\n",
    "split = np.random.rand(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 124) (3, 124)\n",
      "(13, 54) (3, 54)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = split < .7\n",
    "test = split >= .7\n",
    "X_train = X[:, train]\n",
    "Y_train = Y[:, train]\n",
    "X_test = X[:, test]\n",
    "Y_test = Y[:, test]\n",
    "\n",
    "# train = split < .6\n",
    "# cv = (.6 <= split) & (split < .8)\n",
    "# test = .8 <= split\n",
    "\n",
    "# X_train = X[:, train]\n",
    "# X_cv = X[:, cv]\n",
    "# X_test = X[:, test]\n",
    "\n",
    "# Y_train = Y[:, train]\n",
    "# Y_cv = Y[:, cv]\n",
    "# Y_test = Y[:, test]\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "# print(X_cv.shape, Y_cv.shape)\n",
    "print(X_test.shape, Y_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy data in three distinct categories of magnitude, X: [3,30]\n",
    "inputs = X.shape[0] # inputs = 4 for iris\n",
    "m = X.shape[1]\n",
    "# layer_dims = [inputs, 3, 4, 5, 3]\n",
    "layer_dims = [inputs, 3, 8, 5, 3] # best architecture so far\n",
    "# layer_dims = [inputs, 3, 4, 8, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.121700\n",
      "Cost after iteration 10000: 1.041385\n",
      "Cost after iteration 20000: 0.753529\n",
      "Cost after iteration 30000: 0.613838\n",
      "Cost after iteration 40000: 0.593196\n",
      "Cost after iteration 50000: 0.580898\n",
      "Cost after iteration 60000: 0.567976\n",
      "Cost after iteration 70000: 0.553285\n",
      "Cost after iteration 80000: 0.536623\n",
      "Cost after iteration 90000: 0.519056\n",
      "Cost after iteration 100000: 0.503424\n",
      "Cost after iteration 110000: 0.492178\n",
      "Cost after iteration 120000: 0.485194\n",
      "Cost after iteration 130000: 0.481065\n",
      "Cost after iteration 140000: 0.478561\n",
      "Cost after iteration 150000: 0.476922\n",
      "Cost after iteration 160000: 0.475720\n",
      "Cost after iteration 170000: 0.474735\n",
      "Cost after iteration 180000: 0.473853\n",
      "Cost after iteration 190000: 0.473011\n",
      "Cost after iteration 200000: 0.472175\n",
      "Cost after iteration 210000: 0.471323\n",
      "Cost after iteration 220000: 0.470439\n",
      "Cost after iteration 230000: 0.469503\n",
      "Cost after iteration 240000: 0.468488\n",
      "Cost after iteration 250000: 0.467364\n",
      "Cost after iteration 260000: 0.466095\n",
      "Cost after iteration 270000: 0.464621\n",
      "Cost after iteration 280000: 0.462846\n",
      "Cost after iteration 290000: 0.460617\n",
      "Cost after iteration 300000: 0.457718\n",
      "Cost after iteration 310000: 0.453875\n",
      "Cost after iteration 320000: 0.448514\n",
      "Cost after iteration 330000: 0.441287\n",
      "Cost after iteration 340000: 0.431248\n",
      "Cost after iteration 350000: 0.426822\n",
      "Cost after iteration 360000: 0.415389\n",
      "Cost after iteration 370000: 0.347428\n",
      "Cost after iteration 380000: 0.292598\n",
      "Cost after iteration 390000: 0.296309\n",
      "Cost after iteration 400000: 0.256770\n",
      "Cost after iteration 410000: 0.243219\n",
      "Cost after iteration 420000: 0.196156\n",
      "Cost after iteration 430000: 0.165737\n",
      "Cost after iteration 440000: 0.124595\n",
      "Cost after iteration 450000: 0.116257\n",
      "Cost after iteration 460000: 0.177446\n",
      "Cost after iteration 470000: 0.103906\n",
      "Cost after iteration 480000: 0.099296\n",
      "Cost after iteration 490000: 0.125816\n",
      "Cost after iteration 500000: 0.091919\n",
      "Cost after iteration 510000: 0.146253\n",
      "Cost after iteration 520000: 0.108643\n",
      "Cost after iteration 530000: 0.085282\n",
      "Cost after iteration 540000: 0.111173\n",
      "Cost after iteration 550000: 0.106778\n",
      "Cost after iteration 560000: 0.080905\n",
      "Cost after iteration 570000: 0.080201\n",
      "Cost after iteration 580000: 0.080920\n",
      "Cost after iteration 590000: 0.080639\n",
      "Cost after iteration 600000: 0.077152\n",
      "Cost after iteration 610000: 0.079088\n",
      "Cost after iteration 620000: 0.081447\n",
      "Cost after iteration 630000: 0.173287\n",
      "Cost after iteration 640000: 0.083193\n",
      "Cost after iteration 650000: 0.089801\n",
      "Cost after iteration 660000: 0.066613\n",
      "Cost after iteration 670000: 0.081054\n",
      "Cost after iteration 680000: 0.069502\n",
      "Cost after iteration 690000: 0.081692\n",
      "Cost after iteration 700000: 0.067441\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(X_train, Y_train, layer_dims, .001, 700001, True)\n",
    "# iris .001 ?\n",
    "# spiral .033\n",
    "# wine .001 700001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = predict(X_cv, Y_cv, parameters)\n",
    "# print('p.shape', p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (13, 54) Y.shape (3, 54)\n",
      "probas\n",
      " (3, 54)\n",
      "<class 'numpy.ndarray'>\n",
      "Accuracy: 0.9074074074074074\n",
      "p.shape (3, 54)\n"
     ]
    }
   ],
   "source": [
    "p = predict(X_test, Y_test, parameters)\n",
    "print('p.shape', p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (13, 124) Y.shape (3, 124)\n",
      "probas\n",
      " (3, 124)\n",
      "<class 'numpy.ndarray'>\n",
      "Accuracy: 0.967741935483871\n",
      "p.shape (3, 54)\n"
     ]
    }
   ],
   "source": [
    "p_train = predict(X_train, Y_train, parameters)\n",
    "print('p.shape', p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
