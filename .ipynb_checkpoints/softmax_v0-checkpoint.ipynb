{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "# from deep_nn_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "# layers_dims: array containing dimensions of each layer ex initialize_parameters([5, 4, 3])\n",
    "\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in nn\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l-1], layer_dims[l]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l-1], layer_dims[l]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "#     print([(k, parameters[k].shape) for k, v in parameters.items()])\n",
    "    return parameters\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "#     print('\\nlinear forward')\n",
    "#     print('W', W.shape, 'A', A.shape)\n",
    "    Z = np.dot(W.T, A) + b\n",
    "    \n",
    "#     print('Z', Z.shape)\n",
    "    \n",
    "    assert(Z.shape == (W.shape[1], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def softmax(Z):\n",
    "    t = np.exp(Z)\n",
    "    A = t / np.sum(t, axis=0)\n",
    "    cache = Z\n",
    "\n",
    "    return A, cache\n",
    "    \n",
    "\n",
    "def neuron_activation(A_prev, W, b, activation):\n",
    "    if activation == 'sigmoid':\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        # linear cache: A, W, b\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        # activation cache: Z\n",
    "        \n",
    "    elif activation == 'relu':\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    elif activation == 'softmax':\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "        \n",
    "    assert (A.shape == (W.shape[1], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2 # // is floor division\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        \n",
    "        A_prev = A\n",
    "        \n",
    "        A, cache = neuron_activation(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], 'relu')\n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache = neuron_activation(A, parameters['W' + str(L)], parameters['b' + str(L)], 'softmax')\n",
    "    caches.append(cache)\n",
    "#     print('\\n', AL)\n",
    "#     assert(AL.shape == (1, X.shape[1]))\n",
    "#     assert(AL.shape == (3, X.shape[1])) # should be  (classes x m)\n",
    "\n",
    "    return AL, caches\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "#     cost = (-1/m) * np.sum(Y*np.log(AL) + (1-Y)*np.log(1-AL))\n",
    "    \n",
    "#     cost = np.squeeze(cost)\n",
    "#     assert(cost.shape == ())\n",
    "\n",
    "    cost = (-1/m) * np.sum(Y * np.log(AL))\n",
    "    \n",
    "    return cost\n",
    "    \n",
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "#     print('dZ.shape', dZ.shape, 'A_prev.shape', A_prev.shape, 'W.shape', W.shape)\n",
    "#     dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "    dW = (1/m) * np.dot(A_prev, dZ.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "#     dA_prev = np.dot(W.T, dZ)\n",
    "    dA_prev = np.dot(W, dZ)\n",
    "    \n",
    "#     assert (dA_prev.shape == A_prev.shape)\n",
    "#     assert (dW.shape == W.shape)\n",
    "#     assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    # check here for reverse grad shape bug\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "       \n",
    "    elif activation == 'softmax':\n",
    "        dZ = dA\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "#     Y = Y.reshape(AL.shape)\n",
    "    \n",
    "#     dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) #derivative of cost function\n",
    "    dAL = AL - Y\n",
    "#     print('dAL.shape', dAL.shape)\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"softmax\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+2)], current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l+1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l+1)] = dW_temp\n",
    "        grads[\"db\" + str(l+1)] = db_temp\n",
    "            \n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2      \n",
    "#     print('\\nupdate parameters')                               \n",
    "#     print([(k, parameters[k].shape) for k, v in parameters.items()])\n",
    "#     print('grads')\n",
    "#     print([(k, grads[k].shape) for k, v in grads.items()])\n",
    "    for l in range(1, L+1):\n",
    "        parameters['W'+str(l)] = parameters['W'+str(l)] - learning_rate * grads['dW'+str(l)]\n",
    "        parameters['b'+str(l)] = parameters['b'+str(l)] - learning_rate * grads['db'+str(l)]\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    print('I predict...')\n",
    "    print('X', X.shape, 'Y.shape', Y.shape)\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2\n",
    "    p = np.zeros((y.shape[0], m))\n",
    "    acc = []\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "    print('probas\\n', probas.shape)\n",
    "    # max for each example\n",
    "    # search for index == max\n",
    "    # assign 1 to that class and 0 to the others\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        print(probas[:, i])\n",
    "        index = np.where(probas[:, i] == np.max(probas[:, i]))\n",
    "        probas[index, i] = 1\n",
    "        zeros = np.where(probas[:, i] != 1)\n",
    "        probas[zeros, i] = 0\n",
    "        if np.where(probas[:,i] == 1) == np.where(y[:,i] ==1):\n",
    "            acc.append(True)\n",
    "#         print(probas[:, i], 'probas')\n",
    "#         print(y[:, i], 'y')\n",
    "#         print(np.where(probas[:,i] == 1) == np.where(y[:,i] ==1))\n",
    "#         probas[:, i][probas != 1] = 0\n",
    "#         print(index)\n",
    "#         print(np.max(probas[:,i]))\n",
    "#         if probas[0, i] > 0.5:\n",
    "#             p[0, i] = 1\n",
    "#         else:\n",
    "#             p[0, i] = 0\n",
    "    print(type(probas))\n",
    "    print(\"Accuracy: \" + str(sum(acc)/m))\n",
    "#     print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layer_dims, learning_rate = .0075, num_iterations = 10, print_cost = False):\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    \n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        cost = compute_cost(AL, Y)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 10000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 10000 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris(True)\n",
    "iris[0].shape[0]\n",
    "X = iris[0].T\n",
    "# X = X[:, :5]\n",
    "Y_cat = iris[1]\n",
    "Y = []\n",
    "for i in Y_cat:\n",
    "    if i==0:\n",
    "        Y.append([1,0,0])\n",
    "    elif i == 1:\n",
    "        Y.append([0,1,0])\n",
    "    elif i == 2:\n",
    "        Y.append([0,0,1])\n",
    "Y = np.array(Y).T\n",
    "# split into training/test\n",
    "# Y = Y[:, :5]\n",
    "# print('X.shape', X.shape)\n",
    "# print('Y.shape', Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "split = np.random.rand(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = split < .67\n",
    "test = split >= .67\n",
    "X_train = X[:, train]\n",
    "Y_train = Y[:, train]\n",
    "X_test = X[:, test]\n",
    "Y_test = Y[:, test]\n",
    "# split correct? Nope\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (4, 103) X_test (4, 47)\n",
      "Y_train.shape (3, 103) Y_test (3, 47)\n"
     ]
    }
   ],
   "source": [
    "# create dummy data in three distinct categories of magnitude, X: [3,30]\n",
    "inputs = X.shape[0]\n",
    "m = X.shape[1]\n",
    "layer_dims = [inputs, 3, 4, 5, 3]\n",
    "# sm = np.random.rand(inputs, m) * -10000\n",
    "# md = np.random.rand(inputs, m) * 1\n",
    "# lg = np.random.rand(inputs, m) * 10000\n",
    "# X = np.hstack((sm, md, lg))\n",
    "\n",
    "# cat1_truth = np.array([[1], [0], [0]]) \n",
    "# cat2_truth = np.array([[0], [1], [0]])\n",
    "# cat3_truth = np.array([[0], [0], [1]])\n",
    "# Y = np.array([[1], [0], [0]])\n",
    "\n",
    "# # Y: [3, 30] for soft max\n",
    "# for i in range(1, X.shape[1]):\n",
    "#     if i < X.shape[1]/3:\n",
    "#         Y = np.hstack((Y, cat1_truth))\n",
    "#     elif i < X.shape[1] * (2/3):\n",
    "#         Y = np.hstack((Y, cat2_truth))\n",
    "#     elif i < X.shape[1]:\n",
    "#         Y = np.hstack((Y, cat3_truth))\n",
    "print('X_train.shape', X_train.shape, 'X_test', X_test.shape)\n",
    "print('Y_train.shape', Y_train.shape, 'Y_test', Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.098612\n",
      "Cost after iteration 10000: 1.098612\n",
      "Cost after iteration 20000: 1.098612\n",
      "Cost after iteration 30000: 1.098609\n",
      "Cost after iteration 40000: 0.050234\n",
      "Cost after iteration 50000: 0.045491\n",
      "Cost after iteration 60000: 0.042319\n",
      "Cost after iteration 70000: 0.040137\n",
      "Cost after iteration 80000: 0.039944\n",
      "Cost after iteration 90000: 0.039792\n",
      "Cost after iteration 100000: 0.039736\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(X, Y, layer_dims, .1, 110000, True)\n",
    "# cost <= 6.794738"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I predict...\n",
      "X (4, 47) Y.shape (3, 150)\n",
      "probas\n",
      " (3, 47)\n",
      "[9.99995399e-01 4.60103659e-06 3.62712362e-34]\n",
      "[9.99999461e-01 5.38948485e-07 7.48913594e-39]\n",
      "[9.99999190e-01 8.09789538e-07 5.80742198e-38]\n",
      "[9.99999988e-01 1.21609804e-08 3.89946297e-47]\n",
      "[9.99998975e-01 1.02537784e-06 1.90409500e-37]\n",
      "[9.99989836e-01 1.01642399e-05 1.95538418e-32]\n",
      "[9.99990684e-01 9.31605537e-06 1.26140636e-32]\n",
      "[9.99989308e-01 1.06915851e-05 2.52198876e-32]\n",
      "[9.99998450e-01 1.54994255e-06 1.52178945e-36]\n",
      "[9.99999915e-01 8.47609559e-08 6.80799813e-43]\n",
      "[9.99999970e-01 2.97252187e-08 3.49703118e-45]\n",
      "[9.99992247e-01 7.75325851e-06 5.00801797e-33]\n",
      "[6.42982333e-09 9.98516597e-01 1.48339651e-03]\n",
      "[6.99035514e-05 9.99924104e-01 5.99218491e-06]\n",
      "[9.58496719e-06 9.99971549e-01 1.88657583e-05]\n",
      "[6.41009912e-06 9.99969773e-01 2.38169817e-05]\n",
      "[9.95652003e-12 9.33996384e-01 6.60036164e-02]\n",
      "[1.39086871e-13 5.87476458e-01 4.12523542e-01]\n",
      "[7.35469719e-13 7.67894048e-01 2.32105952e-01]\n",
      "[7.77920507e-16 1.32716166e-01 8.67283834e-01]\n",
      "[2.15371375e-09 9.97137253e-01 2.86274461e-03]\n",
      "[7.61441395e-08 9.99665230e-01 3.34694145e-04]\n",
      "[6.99035514e-05 9.99924104e-01 5.99218491e-06]\n",
      "[1.08375187e-05 9.99971592e-01 1.75701883e-05]\n",
      "[1.35286312e-06 9.99939604e-01 5.90435275e-05]\n",
      "[6.99035514e-05 9.99924104e-01 5.99218491e-06]\n",
      "[6.99035514e-05 9.99924104e-01 5.99218491e-06]\n",
      "[6.99035514e-05 9.99924104e-01 5.99218491e-06]\n",
      "[1.59423851e-22 4.41676467e-04 9.99558324e-01]\n",
      "[9.93596077e-28 1.69341776e-05 9.99983066e-01]\n",
      "[8.73213426e-23 3.52187039e-04 9.99647813e-01]\n",
      "[7.65069699e-30 5.14002414e-06 9.99994860e-01]\n",
      "[5.16986169e-26 4.45897867e-05 9.99955410e-01]\n",
      "[6.33927798e-32 1.58842268e-06 9.99998412e-01]\n",
      "[2.01762914e-26 3.54098039e-05 9.99964590e-01]\n",
      "[2.75449958e-22 5.42530500e-04 9.99457469e-01]\n",
      "[1.97653935e-15 1.81764428e-01 8.18235572e-01]\n",
      "[8.93672959e-24 1.57553305e-04 9.99842447e-01]\n",
      "[9.39444638e-13 7.90301162e-01 2.09698838e-01]\n",
      "[1.71593062e-29 6.26483070e-06 9.99993735e-01]\n",
      "[6.50877194e-20 4.23013461e-03 9.95769865e-01]\n",
      "[2.51776387e-30 3.91479527e-06 9.99996085e-01]\n",
      "[1.59423851e-22 4.41676467e-04 9.99558324e-01]\n",
      "[1.51222160e-31 1.96548965e-06 9.99998035e-01]\n",
      "[4.32492383e-26 4.26823830e-05 9.99957318e-01]\n",
      "[1.68815740e-21 1.07275710e-03 9.98927243e-01]\n",
      "[1.58408642e-26 3.33721530e-05 9.99966628e-01]\n",
      "<class 'numpy.ndarray'>\n",
      "Accuracy: 0.9574468085106383\n",
      "p.shape (3, 47)\n"
     ]
    }
   ],
   "source": [
    "p = predict(X_test, Y_test, parameters)\n",
    "print('p.shape', p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7. , 3.2, 4.7, 1.4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
