{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "# import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, preprocessing\n",
    "# from gc_utils import dictionary_to_vector, vector_to_dictionary, gradients_to_vector\n",
    "\n",
    "# from deep_nn_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: gradient_check_n\n",
    "# will need dimensions fixed\n",
    "def dictionary_to_vector(parameters):\n",
    "    \"\"\"\n",
    "    Roll all our parameters dictionary into a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    keys = []\n",
    "    count = 0\n",
    "    for key in [\"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\", \"W4\", \"b4\"]:\n",
    "        \n",
    "        # flatten parameter\n",
    "        new_vector = np.reshape(parameters[key], (-1,1))\n",
    "        keys = keys + [key]*new_vector.shape[0]\n",
    "        \n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "\n",
    "    return theta, keys\n",
    "\n",
    "def vector_to_dictionary(theta):\n",
    "    \"\"\"\n",
    "    Unroll all our parameters dictionary from a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    parameters[\"W1\"] = theta[:12].reshape((3,4))      # 4, 3, 8, 5, 3\n",
    "    parameters[\"b1\"] = theta[12:15].reshape((3,1))\n",
    "    parameters[\"W2\"] = theta[15:39].reshape((8,3))\n",
    "    parameters[\"b2\"] = theta[39:47].reshape((8,1))\n",
    "    parameters[\"W3\"] = theta[47:87].reshape((5,8))\n",
    "    parameters[\"b3\"] = theta[87:92].reshape((5,1))\n",
    "    parameters[\"W4\"] = theta[92:107].reshape((3,5))\n",
    "    parameters[\"b4\"] = theta[107:110].reshape((3,1))\n",
    "    return parameters\n",
    "\n",
    "def gradients_to_vector(gradients):\n",
    "    \"\"\"\n",
    "    Roll all our gradients dictionary into a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    \n",
    "    count = 0\n",
    "    for key in [\"dW1\", \"db1\", \"dW2\", \"db2\", \"dW3\", \"db3\", \"dW4\", \"db4\"]:\n",
    "        # flatten parameter\n",
    "        new_vector = np.reshape(gradients[key], (-1,1))\n",
    "        \n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "\n",
    "    return theta\n",
    "\n",
    "def gradient_check_n(parameters, gradients, X, Y, epsilon = 1e-7):\n",
    "    \"\"\"\n",
    "    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters. \n",
    "    x -- input datapoint, of shape (input size, 1)\n",
    "    y -- true \"label\"\n",
    "    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n",
    "    \n",
    "    Returns:\n",
    "    difference -- difference (2) between the approximated gradient and the backward propagation gradient\n",
    "    \"\"\"\n",
    "\n",
    "    # Set-up variables\n",
    "    parameters_values, _ = dictionary_to_vector(parameters)\n",
    "    grad = gradients_to_vector(gradients)\n",
    "    num_parameters = parameters_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "    \n",
    "    # Compute gradapprox\n",
    "    for i in range(num_parameters):\n",
    "        \n",
    "        # Compute J_plus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_plus[i]\".\n",
    "        # \"_\" is used because the function you have to outputs two parameters but we only care about the first one\n",
    "        ### START CODE HERE ### (approx. 3 lines)\n",
    "        thetaplus = np.copy(parameters_values)                                      # Step 1\n",
    "        thetaplus[i][0] = thetaplus[i][0] + epsilon                                # Step 2\n",
    "        AL_plus, _ = L_model_forward(X, vector_to_dictionary(thetaplus))\n",
    "        J_plus[i] = compute_cost(AL_plus, Y)\n",
    "#         J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))                                   # Step 3\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute J_minus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_minus[i]\".\n",
    "        ### START CODE HERE ### (approx. 3 lines)\n",
    "        thetaminus = np.copy(parameters_values)                                     # Step 1\n",
    "        thetaminus[i][0] = thetaminus[i][0] - epsilon                               # Step 2   \n",
    "        AL_minus, _ = L_model_forward(X, vector_to_dictionary(thetaminus))\n",
    "        J_minus[i] = compute_cost(AL_minus, Y)\n",
    "#         J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus))                                   # Step 3\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute gradapprox[i]\n",
    "        ### START CODE HERE ### (approx. 1 line)\n",
    "        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Compare gradapprox to backward propagation gradients by computing difference.\n",
    "    ### START CODE HERE ### (approx. 1 line)\n",
    "    numerator = np.linalg.norm(grad - gradapprox)                                            # Step 1'\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)                                         # Step 2'\n",
    "    difference = numerator / denominator                                           # Step 3'\n",
    "    ### END CODE HERE ###\n",
    "#     print('grad')\n",
    "#     print(grad)\n",
    "#     print('grad approx')\n",
    "#     print(gradapprox)\n",
    "#     print('numerator', numerator)\n",
    "#     print('denominator', denominator)\n",
    "#     print(grad-gradapprox)\n",
    "    if difference > 1.2e-7:\n",
    "        print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    else:\n",
    "        print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "# layers_dims: array containing dimensions of each layer ex initialize_parameters([5, 4, 3])\n",
    "\n",
    "    np.random.seed(12)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in nn\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "#     print([(k, parameters[k].shape) for k, v in parameters.items()])\n",
    "#     for k, v in parameters.items():\n",
    "#         print(k, v.shape)\n",
    "    return parameters\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "        \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "\n",
    "    assert(A.shape == Z.shape)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def softmax(Z):\n",
    "    t = np.exp(Z)\n",
    "    A = t / np.sum(t, axis=0)\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "    \n",
    "\n",
    "def neuron_activation(A_prev, W, b, activation):\n",
    "    if activation == 'sigmoid':\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        # linear cache: A, W, b\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        # activation cache: Z\n",
    "        \n",
    "    elif activation == 'relu':\n",
    "#         print('relu')\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    elif activation == 'softmax':\n",
    "#         print('softmax')\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "        \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2 # // is floor division\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        \n",
    "        A_prev = A\n",
    "        \n",
    "        A, cache = neuron_activation(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], 'relu')\n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache = neuron_activation(A, parameters['W' + str(L)], parameters['b' + str(L)], 'softmax')\n",
    "    caches.append(cache)\n",
    "#     print('\\n', AL)\n",
    "#     assert(AL.shape == (1, X.shape[1]))\n",
    "    assert(AL.shape == (3, X.shape[1])) # should be  (classes x m)\n",
    "\n",
    "    return AL, caches\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "#     cost = (-1/m) * np.sum(Y*np.log(AL) + (1-Y)*np.log(1-AL))\n",
    "    \n",
    "#     cost = np.squeeze(cost)\n",
    "#     assert(cost.shape == ())\n",
    "#     cost = (-1/m) * np.sum(Y * np.log(AL))\n",
    "    cost = (-1/m) * np.sum(Y * np.log(AL))\n",
    "    \n",
    "    return cost\n",
    "    \n",
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "#     print('dZ.shape', dZ.shape, 'A_prev.shape', A_prev.shape, 'W.shape', W.shape)\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "#     dW = (1/m) * np.dot(A_prev, dZ.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "#     dA_prev = np.dot(W, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1-s) # dLoss/dA * a(1-a) -- same as dLoss/dA * g_prime(Z)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def softmax_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = dA\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    # check here for reverse grad shape bug\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "       \n",
    "    elif activation == 'softmax':\n",
    "        dZ = softmax_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "#     Y = Y.reshape(AL.shape)\n",
    "    \n",
    "#     dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) #derivative of cost function\n",
    "    dAL = AL - Y # this is dZ for softmax, not dA * g_prime(z)\n",
    "    current_cache = caches[L-1]\n",
    "\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"softmax\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+2)], current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l+1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l+1)] = dW_temp\n",
    "        grads[\"db\" + str(l+1)] = db_temp\n",
    "            \n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2      \n",
    "    for l in range(1, L+1):\n",
    "        parameters['W'+str(l)] = parameters['W'+str(l)] - learning_rate * grads['dW'+str(l)]\n",
    "        parameters['b'+str(l)] = parameters['b'+str(l)] - learning_rate * grads['db'+str(l)]\n",
    "#     print('update parameters')\n",
    "#     for k, v in parameters.items():\n",
    "#         print(k, v)\n",
    "    return parameters\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    print('X', X.shape, 'Y.shape', y.shape)\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2\n",
    "    p = np.zeros((y.shape[0], m))\n",
    "    acc = []\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "    print('probas\\n', probas.shape)\n",
    "    # max for each example\n",
    "    # search for index == max\n",
    "    # assign 1 to that class and 0 to the others\n",
    "    for i in range(0, probas.shape[1]):\n",
    "#         print(probas[:, i])\n",
    "        index = np.where(probas[:, i] == np.max(probas[:, i]))\n",
    "        probas[index, i] = 1\n",
    "        zeros = np.where(probas[:, i] != 1)\n",
    "        probas[zeros, i] = 0\n",
    "        if np.where(probas[:,i] == 1) == np.where(y[:,i] ==1):\n",
    "            acc.append(True)\n",
    "            \n",
    "    print(type(probas))\n",
    "    print(\"Accuracy: \" + str(sum(acc)/m))\n",
    "#     print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layer_dims, learning_rate = .0075, num_iterations = 10, print_cost = False):\n",
    "    np.random.seed(3)\n",
    "    costs = []\n",
    "    \n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        cost = compute_cost(AL, Y)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 10000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "#             difference = gradient_check_n(parameters, grads, X, Y)\n",
    "#             print(difference)\n",
    "        if print_cost and i % 10000 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = datasets.load_wine(True)\n",
    "wine[0].shape[0]\n",
    "X = wine[0].T\n",
    "X = preprocessing.scale(X)\n",
    "\n",
    "Y_cat = wine[1]\n",
    "Y = []\n",
    "for i in Y_cat:\n",
    "    if i==0:\n",
    "        Y.append([1,0,0])\n",
    "    elif i == 1:\n",
    "        Y.append([0,1,0])\n",
    "    elif i == 2:\n",
    "        Y.append([0,0,1])\n",
    "Y = np.array(Y).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris = datasets.load_iris(True)\n",
    "# iris[0].shape[0]\n",
    "# X = iris[0].T\n",
    "# X = preprocessing.scale(X)\n",
    "\n",
    "# Y_cat = iris[1]\n",
    "# Y = []\n",
    "# for i in Y_cat:\n",
    "#     if i==0:\n",
    "#         Y.append([1,0,0])\n",
    "#     elif i == 1:\n",
    "#         Y.append([0,1,0])\n",
    "#     elif i == 2:\n",
    "#         Y.append([0,0,1])\n",
    "# Y = np.array(Y).T\n",
    "# split into training/test\n",
    "# Y = Y[:, :5]\n",
    "# print('X.shape', X.shape)\n",
    "# print('Y.shape', Y.shape)\n",
    "# iris\n",
    "#normalize data\n",
    "# l2 regularization\n",
    "# gradient checking\n",
    "# X.mean(axis=0)\n",
    "# X.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 3000 # number of points per class\n",
    "# D = 2 # dimensionality\n",
    "# K = 3 # number of classes\n",
    "# X = np.zeros((N*K,D)) # data matrix (each row = single example)\n",
    "# y = np.zeros(N*K, dtype='uint8') # class labels\n",
    "# for j in range(K):\n",
    "#   ix = range(N*j,N*(j+1))\n",
    "#   r = np.linspace(0.0,1,N) # radius\n",
    "#   t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
    "#   X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "#   y[ix] = j\n",
    "# # lets visualize the data:\n",
    "# plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "# plt.show()\n",
    "# X = X.T\n",
    "# Y = []\n",
    "# for i in y:\n",
    "#     if i==0:\n",
    "#         Y.append([1,0,0])\n",
    "#     elif i == 1:\n",
    "#         Y.append([0,1,0])\n",
    "#     elif i == 2:\n",
    "#         Y.append([0,0,1])\n",
    "# Y = np.array(Y).T\n",
    "# print(Y.shape, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(101)\n",
    "split = np.random.rand(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 124) (3, 124)\n",
      "(13, 54) (3, 54)\n",
      "[-0.28151961 -0.31819362 -0.31649466 -0.28090448 -0.04246487 -0.3163482\n",
      " -0.31628962 -0.32279251 -0.3178714  -0.30829281 -0.32068347 -0.31318463\n",
      "  3.45503988]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = split < .7\n",
    "test = split >= .7\n",
    "X_train = X[:, train]\n",
    "Y_train = Y[:, train]\n",
    "X_test = X[:, test]\n",
    "Y_test = Y[:, test]\n",
    "\n",
    "# train = split < .6\n",
    "# cv = (.6 <= split) & (split < .8)\n",
    "# test = .8 <= split\n",
    "\n",
    "# X_train = X[:, train]\n",
    "# X_cv = X[:, cv]\n",
    "# X_test = X[:, test]\n",
    "\n",
    "# Y_train = Y[:, train]\n",
    "# Y_cv = Y[:, cv]\n",
    "# Y_test = Y[:, test]\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "# print(X_cv.shape, Y_cv.shape)\n",
    "print(X_test.shape, Y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy data in three distinct categories of magnitude, X: [3,30]\n",
    "inputs = X.shape[0] # inputs = 4 for iris\n",
    "m = X.shape[1]\n",
    "# layer_dims = [inputs, 3, 4, 5, 3]\n",
    "layer_dims = [inputs, 3, 8, 5, 3] # best architecture so far\n",
    "# layer_dims = [inputs, 3, 4, 8, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.121700\n",
      "Cost after iteration 10000: 1.041385\n",
      "Cost after iteration 20000: 0.753529\n",
      "Cost after iteration 30000: 0.613838\n",
      "Cost after iteration 40000: 0.593196\n",
      "Cost after iteration 50000: 0.580898\n",
      "Cost after iteration 60000: 0.567976\n",
      "Cost after iteration 70000: 0.553285\n",
      "Cost after iteration 80000: 0.536623\n",
      "Cost after iteration 90000: 0.519056\n",
      "Cost after iteration 100000: 0.503424\n",
      "Cost after iteration 110000: 0.492178\n",
      "Cost after iteration 120000: 0.485194\n",
      "Cost after iteration 130000: 0.481065\n",
      "Cost after iteration 140000: 0.478561\n",
      "Cost after iteration 150000: 0.476922\n",
      "Cost after iteration 160000: 0.475720\n",
      "Cost after iteration 170000: 0.474735\n",
      "Cost after iteration 180000: 0.473853\n",
      "Cost after iteration 190000: 0.473011\n",
      "Cost after iteration 200000: 0.472175\n",
      "Cost after iteration 210000: 0.471323\n",
      "Cost after iteration 220000: 0.470439\n",
      "Cost after iteration 230000: 0.469503\n",
      "Cost after iteration 240000: 0.468488\n",
      "Cost after iteration 250000: 0.467364\n",
      "Cost after iteration 260000: 0.466095\n",
      "Cost after iteration 270000: 0.464621\n",
      "Cost after iteration 280000: 0.462846\n",
      "Cost after iteration 290000: 0.460617\n",
      "Cost after iteration 300000: 0.457718\n",
      "Cost after iteration 310000: 0.453875\n",
      "Cost after iteration 320000: 0.448514\n",
      "Cost after iteration 330000: 0.441287\n",
      "Cost after iteration 340000: 0.431248\n",
      "Cost after iteration 350000: 0.426822\n",
      "Cost after iteration 360000: 0.415389\n",
      "Cost after iteration 370000: 0.347428\n",
      "Cost after iteration 380000: 0.292598\n",
      "Cost after iteration 390000: 0.296309\n",
      "Cost after iteration 400000: 0.256770\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(X_train, Y_train, layer_dims, .001, 400001, True)\n",
    "# iris .001 ?\n",
    "# spiral .033\n",
    "# wine .001 700001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = predict(X_cv, Y_cv, parameters)\n",
    "# print('p.shape', p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (13, 54) Y.shape (3, 54)\n",
      "probas\n",
      " (3, 54)\n",
      "<class 'numpy.ndarray'>\n",
      "Accuracy: 0.8888888888888888\n",
      "p.shape (3, 54)\n"
     ]
    }
   ],
   "source": [
    "p = predict(X_test, Y_test, parameters)\n",
    "print('p.shape', p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (13, 124) Y.shape (3, 124)\n",
      "probas\n",
      " (3, 124)\n",
      "<class 'numpy.ndarray'>\n",
      "Accuracy: 0.8790322580645161\n",
      "p.shape (3, 54)\n"
     ]
    }
   ],
   "source": [
    "p_train = predict(X_train, Y_train, parameters)\n",
    "print('p.shape', p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
