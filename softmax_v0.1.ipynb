{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "# import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, preprocessing\n",
    "\n",
    "# from deep_nn_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "# layers_dims: array containing dimensions of each layer ex initialize_parameters([5, 4, 3])\n",
    "\n",
    "    np.random.seed(12)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in nn\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l-1], layer_dims[l]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l-1], layer_dims[l]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "#     print([(k, parameters[k].shape) for k, v in parameters.items()])\n",
    "    return parameters\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "#     print('\\nlinear forward')\n",
    "#     print('W', W.shape, 'A', A.shape)\n",
    "    Z = np.dot(W.T, A) + b\n",
    "    \n",
    "#     print('Z', Z.shape)\n",
    "    \n",
    "    assert(Z.shape == (W.shape[1], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def softmax(Z):\n",
    "    t = np.exp(Z)\n",
    "    A = t / np.sum(t, axis=0)\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "    \n",
    "\n",
    "def neuron_activation(A_prev, W, b, activation):\n",
    "    if activation == 'sigmoid':\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        # linear cache: A, W, b\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        # activation cache: Z\n",
    "        \n",
    "    elif activation == 'relu':\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    elif activation == 'softmax':\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "        \n",
    "    assert (A.shape == (W.shape[1], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2 # // is floor division\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        \n",
    "        A_prev = A\n",
    "        \n",
    "        A, cache = neuron_activation(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], 'relu')\n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache = neuron_activation(A, parameters['W' + str(L)], parameters['b' + str(L)], 'softmax')\n",
    "    caches.append(cache)\n",
    "#     print('\\n', AL)\n",
    "#     assert(AL.shape == (1, X.shape[1]))\n",
    "#     assert(AL.shape == (3, X.shape[1])) # should be  (classes x m)\n",
    "\n",
    "    return AL, caches\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "#     cost = (-1/m) * np.sum(Y*np.log(AL) + (1-Y)*np.log(1-AL))\n",
    "    \n",
    "#     cost = np.squeeze(cost)\n",
    "#     assert(cost.shape == ())\n",
    "    cost = (-1/m) * np.sum(Y * np.log(AL))\n",
    "    \n",
    "    return cost\n",
    "    \n",
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "#     print('dZ.shape', dZ.shape, 'A_prev.shape', A_prev.shape, 'W.shape', W.shape)\n",
    "#     dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "    dW = (1/m) * np.dot(A_prev, dZ.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "#     dA_prev = np.dot(W.T, dZ)\n",
    "    dA_prev = np.dot(W, dZ)\n",
    "    \n",
    "#     assert (dA_prev.shape == A_prev.shape)\n",
    "#     assert (dW.shape == W.shape)\n",
    "#     assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1-s) # dLoss/dA * a(1-a) -- same as dLoss/dA * g_prime(Z)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    # check here for reverse grad shape bug\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "       \n",
    "    elif activation == 'softmax':\n",
    "        dZ = dA\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "#     Y = Y.reshape(AL.shape)\n",
    "    \n",
    "#     dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) #derivative of cost function\n",
    "    dAL = AL - Y # this is dZ for softmax, not dA * g_prime(z)\n",
    "#     print('dAL.shape', dAL.shape)\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"softmax\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+2)], current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l+1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l+1)] = dW_temp\n",
    "        grads[\"db\" + str(l+1)] = db_temp\n",
    "            \n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2      \n",
    "#     print('\\nupdate parameters')                               \n",
    "#     print([(k, parameters[k].shape) for k, v in parameters.items()])\n",
    "#     print('grads')\n",
    "#     print([(k, grads[k].shape) for k, v in grads.items()])\n",
    "    for l in range(1, L+1):\n",
    "        parameters['W'+str(l)] = parameters['W'+str(l)] - learning_rate * grads['dW'+str(l)]\n",
    "        parameters['b'+str(l)] = parameters['b'+str(l)] - learning_rate * grads['db'+str(l)]\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    print('X', X.shape, 'Y.shape', y.shape)\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2\n",
    "    p = np.zeros((y.shape[0], m))\n",
    "    acc = []\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "    print('probas\\n', probas.shape)\n",
    "    # max for each example\n",
    "    # search for index == max\n",
    "    # assign 1 to that class and 0 to the others\n",
    "    for i in range(0, probas.shape[1]):\n",
    "#         print(probas[:, i])\n",
    "        index = np.where(probas[:, i] == np.max(probas[:, i]))\n",
    "        probas[index, i] = 1\n",
    "        zeros = np.where(probas[:, i] != 1)\n",
    "        probas[zeros, i] = 0\n",
    "        if np.where(probas[:,i] == 1) == np.where(y[:,i] ==1):\n",
    "            acc.append(True)\n",
    "#         print(probas[:, i], 'probas')\n",
    "#         print(y[:, i], 'y')\n",
    "#         print(np.where(probas[:,i] == 1) == np.where(y[:,i] ==1))\n",
    "#         probas[:, i][probas != 1] = 0\n",
    "#         print(index)\n",
    "#         print(np.max(probas[:,i]))\n",
    "#         if probas[0, i] > 0.5:\n",
    "#             p[0, i] = 1\n",
    "#         else:\n",
    "#             p[0, i] = 0\n",
    "    print(type(probas))\n",
    "    print(\"Accuracy: \" + str(sum(acc)/m))\n",
    "#     print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layer_dims, learning_rate = .0075, num_iterations = 10, print_cost = False):\n",
    "    np.random.seed(3)\n",
    "    costs = []\n",
    "    \n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        cost = compute_cost(AL, Y)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 10000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 10000 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = datasets.load_wine(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris(True)\n",
    "iris[0].shape[0]\n",
    "X = iris[0].T\n",
    "X = preprocessing.scale(X)\n",
    "\n",
    "Y_cat = iris[1]\n",
    "Y = []\n",
    "for i in Y_cat:\n",
    "    if i==0:\n",
    "        Y.append([1,0,0])\n",
    "    elif i == 1:\n",
    "        Y.append([0,1,0])\n",
    "    elif i == 2:\n",
    "        Y.append([0,0,1])\n",
    "Y = np.array(Y).T\n",
    "# split into training/test\n",
    "# Y = Y[:, :5]\n",
    "# print('X.shape', X.shape)\n",
    "# print('Y.shape', Y.shape)\n",
    "# iris\n",
    "#normalize data\n",
    "# l2 regularization\n",
    "# gradient checking\n",
    "# X.mean(axis=0)\n",
    "# X.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "split = np.random.rand(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 112) (3, 112)\n",
      "(4, 38) (3, 38)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = split < .7\n",
    "test = split >= .7\n",
    "X_train = X[:, train]\n",
    "Y_train = Y[:, train]\n",
    "X_test = X[:, test]\n",
    "Y_test = Y[:, test]\n",
    "\n",
    "# train = split < .6\n",
    "# cv = (.6 <= split) & (split < .8)\n",
    "# test = .8 <= split\n",
    "\n",
    "# X_train = X[:, train]\n",
    "# X_cv = X[:, cv]\n",
    "# X_test = X[:, test]\n",
    "\n",
    "# Y_train = Y[:, train]\n",
    "# Y_cv = Y[:, cv]\n",
    "# Y_test = Y[:, test]\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "# print(X_cv.shape, Y_cv.shape)\n",
    "print(X_test.shape, Y_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy data in three distinct categories of magnitude, X: [3,30]\n",
    "inputs = X.shape[0]\n",
    "m = X.shape[1]\n",
    "# layer_dims = [inputs, 3, 4, 5, 3]\n",
    "layer_dims = [inputs, 3, 8, 5, 3] # best architecture so far\n",
    "# layer_dims = [inputs, 3, 4, 8, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.098612\n",
      "Cost after iteration 10000: 1.097567\n",
      "Cost after iteration 20000: 1.097567\n",
      "Cost after iteration 30000: 1.097567\n",
      "Cost after iteration 40000: 1.097567\n",
      "Cost after iteration 50000: 1.097567\n",
      "Cost after iteration 60000: 1.097566\n",
      "Cost after iteration 70000: 0.119498\n",
      "Cost after iteration 80000: 0.119427\n",
      "Cost after iteration 90000: 0.118178\n",
      "Cost after iteration 100000: 0.117397\n",
      "Cost after iteration 110000: 0.116240\n",
      "Cost after iteration 120000: 0.072019\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(X_train, Y_train, layer_dims, .1, 130000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = predict(X_cv, Y_cv, parameters)\n",
    "# print('p.shape', p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (4, 38) Y.shape (3, 38)\n",
      "probas\n",
      " (3, 38)\n",
      "<class 'numpy.ndarray'>\n",
      "Accuracy: 0.9473684210526315\n",
      "p.shape (3, 38)\n"
     ]
    }
   ],
   "source": [
    "p = predict(X_test, Y_test, parameters)\n",
    "print('p.shape', p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (4, 112) Y.shape (3, 112)\n",
      "probas\n",
      " (3, 112)\n",
      "<class 'numpy.ndarray'>\n",
      "Accuracy: 0.9821428571428571\n",
      "p.shape (3, 38)\n"
     ]
    }
   ],
   "source": [
    "p_train = predict(X_train, Y_train, parameters)\n",
    "print('p.shape', p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
