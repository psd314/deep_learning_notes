{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "# import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, preprocessing\n",
    "\n",
    "# from deep_nn_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "# layers_dims: array containing dimensions of each layer ex initialize_parameters([5, 4, 3])\n",
    "\n",
    "    np.random.seed(12)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in nn\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l-1], layer_dims[l]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l-1], layer_dims[l]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "#     print([(k, parameters[k].shape) for k, v in parameters.items()])\n",
    "    return parameters\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "#     print('\\nlinear forward')\n",
    "#     print('W', W.shape, 'A', A.shape)\n",
    "    Z = np.dot(W.T, A) + b\n",
    "    \n",
    "#     print('Z', Z.shape)\n",
    "    \n",
    "    assert(Z.shape == (W.shape[1], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def softmax(Z):\n",
    "    t = np.exp(Z)\n",
    "    A = t / np.sum(t, axis=0)\n",
    "    cache = Z\n",
    "\n",
    "    return A, cache\n",
    "    \n",
    "\n",
    "def neuron_activation(A_prev, W, b, activation):\n",
    "    if activation == 'sigmoid':\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        # linear cache: A, W, b\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        # activation cache: Z\n",
    "        \n",
    "    elif activation == 'relu':\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    elif activation == 'softmax':\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "        \n",
    "    assert (A.shape == (W.shape[1], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2 # // is floor division\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        \n",
    "        A_prev = A\n",
    "        \n",
    "        A, cache = neuron_activation(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], 'relu')\n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache = neuron_activation(A, parameters['W' + str(L)], parameters['b' + str(L)], 'softmax')\n",
    "    caches.append(cache)\n",
    "#     print('\\n', AL)\n",
    "#     assert(AL.shape == (1, X.shape[1]))\n",
    "#     assert(AL.shape == (3, X.shape[1])) # should be  (classes x m)\n",
    "\n",
    "    return AL, caches\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "#     cost = (-1/m) * np.sum(Y*np.log(AL) + (1-Y)*np.log(1-AL))\n",
    "    \n",
    "#     cost = np.squeeze(cost)\n",
    "#     assert(cost.shape == ())\n",
    "\n",
    "    cost = (-1/m) * np.sum(Y * np.log(AL))\n",
    "    \n",
    "    return cost\n",
    "    \n",
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "#     print('dZ.shape', dZ.shape, 'A_prev.shape', A_prev.shape, 'W.shape', W.shape)\n",
    "#     dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "    dW = (1/m) * np.dot(A_prev, dZ.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "#     dA_prev = np.dot(W.T, dZ)\n",
    "    dA_prev = np.dot(W, dZ)\n",
    "    \n",
    "#     assert (dA_prev.shape == A_prev.shape)\n",
    "#     assert (dW.shape == W.shape)\n",
    "#     assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    # check here for reverse grad shape bug\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "       \n",
    "    elif activation == 'softmax':\n",
    "        dZ = dA\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "#     Y = Y.reshape(AL.shape)\n",
    "    \n",
    "#     dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) #derivative of cost function\n",
    "    dAL = AL - Y\n",
    "#     print('dAL.shape', dAL.shape)\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"softmax\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+2)], current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l+1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l+1)] = dW_temp\n",
    "        grads[\"db\" + str(l+1)] = db_temp\n",
    "            \n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2      \n",
    "#     print('\\nupdate parameters')                               \n",
    "#     print([(k, parameters[k].shape) for k, v in parameters.items()])\n",
    "#     print('grads')\n",
    "#     print([(k, grads[k].shape) for k, v in grads.items()])\n",
    "    for l in range(1, L+1):\n",
    "        parameters['W'+str(l)] = parameters['W'+str(l)] - learning_rate * grads['dW'+str(l)]\n",
    "        parameters['b'+str(l)] = parameters['b'+str(l)] - learning_rate * grads['db'+str(l)]\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    print('X', X.shape, 'Y.shape', y.shape)\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2\n",
    "    p = np.zeros((y.shape[0], m))\n",
    "    acc = []\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "    print('probas\\n', probas.shape)\n",
    "    # max for each example\n",
    "    # search for index == max\n",
    "    # assign 1 to that class and 0 to the others\n",
    "    for i in range(0, probas.shape[1]):\n",
    "#         print(probas[:, i])\n",
    "        index = np.where(probas[:, i] == np.max(probas[:, i]))\n",
    "        probas[index, i] = 1\n",
    "        zeros = np.where(probas[:, i] != 1)\n",
    "        probas[zeros, i] = 0\n",
    "        if np.where(probas[:,i] == 1) == np.where(y[:,i] ==1):\n",
    "            acc.append(True)\n",
    "#         print(probas[:, i], 'probas')\n",
    "#         print(y[:, i], 'y')\n",
    "#         print(np.where(probas[:,i] == 1) == np.where(y[:,i] ==1))\n",
    "#         probas[:, i][probas != 1] = 0\n",
    "#         print(index)\n",
    "#         print(np.max(probas[:,i]))\n",
    "#         if probas[0, i] > 0.5:\n",
    "#             p[0, i] = 1\n",
    "#         else:\n",
    "#             p[0, i] = 0\n",
    "    print(type(probas))\n",
    "    print(\"Accuracy: \" + str(sum(acc)/m))\n",
    "#     print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layer_dims, learning_rate = .0075, num_iterations = 10, print_cost = False):\n",
    "    np.random.seed(3)\n",
    "    costs = []\n",
    "    \n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        cost = compute_cost(AL, Y)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 10000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 10000 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = datasets.load_wine(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris(True)\n",
    "iris[0].shape[0]\n",
    "X = iris[0].T\n",
    "X = preprocessing.scale(X)\n",
    "\n",
    "Y_cat = iris[1]\n",
    "Y = []\n",
    "for i in Y_cat:\n",
    "    if i==0:\n",
    "        Y.append([1,0,0])\n",
    "    elif i == 1:\n",
    "        Y.append([0,1,0])\n",
    "    elif i == 2:\n",
    "        Y.append([0,0,1])\n",
    "Y = np.array(Y).T\n",
    "# split into training/test\n",
    "# Y = Y[:, :5]\n",
    "# print('X.shape', X.shape)\n",
    "# print('Y.shape', Y.shape)\n",
    "# iris\n",
    "#normalize data\n",
    "# l2 regularization\n",
    "# gradient checking\n",
    "# X.mean(axis=0)\n",
    "# X.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "split = np.random.rand(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 112) (3, 112)\n",
      "(4, 38) (3, 38)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = split < .7\n",
    "test = split >= .7\n",
    "X_train = X[:, train]\n",
    "Y_train = Y[:, train]\n",
    "X_test = X[:, test]\n",
    "Y_test = Y[:, test]\n",
    "\n",
    "# train = split < .6\n",
    "# cv = (.6 <= split) & (split < .8)\n",
    "# test = .8 <= split\n",
    "\n",
    "# X_train = X[:, train]\n",
    "# X_cv = X[:, cv]\n",
    "# X_test = X[:, test]\n",
    "\n",
    "# Y_train = Y[:, train]\n",
    "# Y_cv = Y[:, cv]\n",
    "# Y_test = Y[:, test]\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "# print(X_cv.shape, Y_cv.shape)\n",
    "print(X_test.shape, Y_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy data in three distinct categories of magnitude, X: [3,30]\n",
    "inputs = X.shape[0]\n",
    "m = X.shape[1]\n",
    "# layer_dims = [inputs, 3, 4, 5, 3]\n",
    "layer_dims = [inputs, 3, 8, 5, 3] # best architecture so far\n",
    "# layer_dims = [inputs, 3, 4, 8, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.098612\n",
      "Cost after iteration 10000: 1.097567\n",
      "Cost after iteration 20000: 1.097567\n",
      "Cost after iteration 30000: 1.097567\n",
      "Cost after iteration 40000: 1.097567\n",
      "Cost after iteration 50000: 1.097567\n",
      "Cost after iteration 60000: 1.097566\n",
      "Cost after iteration 70000: 0.119498\n",
      "Cost after iteration 80000: 0.119427\n",
      "Cost after iteration 90000: 0.118178\n",
      "Cost after iteration 100000: 0.117397\n",
      "Cost after iteration 110000: 0.116240\n",
      "Cost after iteration 120000: 0.072019\n",
      "Cost after iteration 130000: 0.070047\n",
      "Cost after iteration 140000: 0.126870\n",
      "Cost after iteration 150000: 0.117915\n",
      "Cost after iteration 160000: 0.117430\n",
      "Cost after iteration 170000: 0.117155\n",
      "Cost after iteration 180000: 0.117151\n",
      "Cost after iteration 190000: 0.117839\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(X_train, Y_train, layer_dims, .1, 200000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = predict(X_cv, Y_cv, parameters)\n",
    "# print('p.shape', p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (4, 38) Y.shape (3, 38)\n",
      "probas\n",
      " (3, 38)\n",
      "<class 'numpy.ndarray'>\n",
      "Accuracy: 0.9736842105263158\n",
      "p.shape (3, 38)\n"
     ]
    }
   ],
   "source": [
    "p = predict(X_test, Y_test, parameters)\n",
    "print('p.shape', p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (4, 112) Y.shape (3, 112)\n",
      "probas\n",
      " (3, 112)\n",
      "<class 'numpy.ndarray'>\n",
      "Accuracy: 0.9642857142857143\n",
      "p.shape (3, 38)\n"
     ]
    }
   ],
   "source": [
    "p_train = predict(X_train, Y_train, parameters)\n",
    "print('p.shape', p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
